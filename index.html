<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="GESCAM DATASET"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="GESCAM">
  <meta name="twitter:description" content="A Dataset and Method on Gaze Estimation for Classroom Attention Measurement">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>GESCAM : A Dataset and Method on Gaze Estimation for Classroom Attention Measurement</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">GESCAM : A Dataset and Method on Gaze Estimation for Classroom Attention Measurement</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/athulmathew/" target="_blank">Athul M. Mathew</a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.co.uk/citations?user=539QhJ8AAAAJ&hl=en" target="_blank">Arshad Ali Khan</a>,</span>
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/thariqkhalid-deeplearning" target="_blank">Thariq Khalid</a>,</span>
                    <span class="author-block">
                    <a href="Fourth AUTHOR PERSONAL LINK" target="_blank">Riad Souissi</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Elm Company<br>CVPR GAZE 2024</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Mathew_GESCAM__A_Dataset_and_Method_on_Gaze_Estimation_for_CVPRW_2024_paper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/Classroom_06_video_01_final2 - Trim.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Human gaze provides crucial insights into individual attention during social or educational interactions. Attention systems often rely on head and facial features to predict gaze direction, but reliable gaze target detection (GTD) requires rich contextual cues. These cues inform the system about an individual's position within a scene and the surrounding objects they might be interacting with. Our paper proposes attention measurement using GTD in educational classrooms, leveraging a synthetic dataset called GESCAM (Gaze Estimation based Synthetic Classroom Attention Measurement). This dataset was meticulously generated using 3D modelling, animation, simulation, and rendering techniques comprising 60,000 images with 650,000 instances of individuals (students, teachers) engaged in various activities, including looking at blackboard, notebooks, mobile phones etc. Our novel network trained on GESCAM proficiently identifies gaze fixations within complex classroom scenes, offering insights into human attention in classrooms across diverse contexts.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/classroom11_cam1.PNG" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          RGB image of classroom
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/classroom11_cam1_depth.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Depth map for each corresponding RGB image
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/classroom11_cam1_with_bounding_boxes.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Annotation of bounding boxes and labels for people and various objects within the scene
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/classroom11_cam1_with_object_mask.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Masks highlighting individual objects for precise identification
      </h2>
    </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/classroom11_cam1_with_gaze_lines.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Gaze lines indicating points of interests within the scene
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!--Dataset -->
  <section class="section" id="Dataset">
    <div class="container is-max-desktop content">
      <h2 class="title">Dataset</h2>
        <h2 style="font-size: 14px;">Get a Sample of the GESCAM Dataset</h2>
        <p>You can download a subset of the annotated GESCAM dataset <a href="https://data.mendeley.com/datasets/76s5pymmp3/1">here</a>.</p>
        <h2 style="font-size: 14px;">Looking for the Full Dataset?</h2>
        <p>For access to the complete GESCAM dataset, please contact us at:</p>
        <ul>
          <li><a href="mailto:amathew@elm.sa">amathew@elm.sa</a></li>
          <li><a href="mailto:arkhan@elm.sa">arkhan@elm.sa</a></li>
          <li><a href="mailto:tkadavil@elm.sa">tkadavil@elm.sa</a></li>
        </ul>
    </div>
</section>
<!--End Dataset -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
        <p>
        If you find this work useful, please cite:
        </p>
          <pre><code>@InProceedings{Mathew_2024_CVPR,
    author    = {Mathew, Athul M. and Khan, Arshad Ali and Khalid, Thariq and Souissi, Riad},
    title     = {GESCAM : A Dataset and Method on Gaze Estimation for Classroom Attention Measurement},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2024},
    pages     = {636-645}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Default Statcounter code for GESCAM
https://athulmmathew.github.io/GESCAM/ -->
<script type="text/javascript">
var sc_project=12988525; 
var sc_invisible=0; 
var sc_security="d6b02b53"; 
var scJsHost = "https://";
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12988525/0/d6b02b53/0/"
alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->
<a href="https://statcounter.com/p12988525/?guest=1">View My
Stats</a>

  </body>
  </html>
